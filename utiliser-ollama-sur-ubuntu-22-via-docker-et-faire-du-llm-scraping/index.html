<!DOCTYPE html>
<html lang="fr">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author"
  content="Olivier Eeckhoutte">
<meta name="description"
  content="Dans cet article, nous allons voir comment utiliser Ollama sur Ubuntu 22 via Docker tout en tirant parti du GPU connect√© au serveur dans le but de faire du scraping en posant des questions √† son LLM auto-h√©berg√©.
Pr√©-requis
nvidia-smi d√©j√† install√© Cuda install√© Docker install√© Installation du NVIDIA Container Toolkit
sudo apt install -y nvidia-container-toolkit sudo nvidia-ctk runtime configure --runtime=docker sudo systemctl restart docker # V√©rification docker run --rm --gpus all nvidia/cuda:12." />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<script>
  
  
  if (!(window.location.host.startsWith("127.0.0.1")) && !(window.location.host.startsWith("localhost"))) {
    if (window.location.protocol != "https:") {
      console.log("Redirecting to https...")
      window.location.protocol = "https";
    }
  }
</script>


<link rel="canonical" href="https://leandeep.com/utiliser-ollama-sur-ubuntu-22-via-docker-et-faire-du-llm-scraping/" />



<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

<style>
  #app {
    display: none;
    border-radius: 10px;
    box-shadow: 2px 5px 12px -1px rgba(0, 0, 0, 0.56);
    padding: 20px;
    background-color: white;
    max-width: 500px;
    margin: 15px auto;
    text-align: center;
    min-height: 500px;
  }

  #app input {
    margin: 0 auto;
    float: none;
    width: 100%;
    max-width: 300px;
    padding: 5px 10px;
    border: 2px solid black;
  }

  #app ul {
    margin: 0;
    padding: 0;
  }

  #app li {
    text-align: left;
    padding: 5px 10px;
    width: 100%;
    max-width: 280px;
    margin: 1px auto;
    background-color: white;
    border: 1px solid black;
    list-style: none;
  }
</style>




<title>
  
  Utiliser Ollama sur Ubuntu 22 via Docker et faire du LLM scraping :: Lean Deep Tech blog 
  
</title>



<link href="//cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
  type="text/css">



<link rel="stylesheet" href="https://leandeep.com/main.min.44eaa49e743eabd51724579f1d2ece0dac6f56215301d6961ca74092199d4a05.css">




    <link rel="apple-touch-icon" sizes="180x180" href="https://leandeep.com/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://leandeep.com/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://leandeep.com/favicon-16x16.png">
    <link rel="manifest" href="https://leandeep.com/site.webmanifest">
    <link rel="mask-icon" href="https://leandeep.com/safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="https://leandeep.com/favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">

<meta itemprop="name" content="Utiliser Ollama sur Ubuntu 22 via Docker et faire du LLM scraping">
<meta itemprop="description" content="Dans cet article, nous allons voir comment utiliser Ollama sur Ubuntu 22 via Docker tout en tirant parti du GPU connect√© au serveur dans le but de faire du scraping en posant des questions √† son LLM auto-h√©berg√©.
Pr√©-requis
nvidia-smi d√©j√† install√© Cuda install√© Docker install√© Installation du NVIDIA Container Toolkit
sudo apt install -y nvidia-container-toolkit sudo nvidia-ctk runtime configure --runtime=docker sudo systemctl restart docker # V√©rification docker run --rm --gpus all nvidia/cuda:12."><meta itemprop="datePublished" content="2025-05-04T23:32:00+02:00" />
<meta itemprop="dateModified" content="2025-05-04T23:32:00+02:00" />
<meta itemprop="wordCount" content="326"><meta itemprop="image" content="https://leandeep.com"/>
<meta itemprop="keywords" content="Linux,LLM,AI," /><meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://leandeep.com"/>

<meta name="twitter:title" content="Utiliser Ollama sur Ubuntu 22 via Docker et faire du LLM scraping"/>
<meta name="twitter:description" content="Dans cet article, nous allons voir comment utiliser Ollama sur Ubuntu 22 via Docker tout en tirant parti du GPU connect√© au serveur dans le but de faire du scraping en posant des questions √† son LLM auto-h√©berg√©.
Pr√©-requis
nvidia-smi d√©j√† install√© Cuda install√© Docker install√© Installation du NVIDIA Container Toolkit
sudo apt install -y nvidia-container-toolkit sudo nvidia-ctk runtime configure --runtime=docker sudo systemctl restart docker # V√©rification docker run --rm --gpus all nvidia/cuda:12."/>





<meta property="article:published_time" content="2025-05-04 23:32:00 &#43;0200 CEST" />







    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://leandeep.com/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">$ cd /home/leandeep</span>
            <span class="logo__cursor" style=""></span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://leandeep.com/events/">Featured Articles</a></li><li><a href="https://leandeep.com/posts/">All Articles</a></li><li><a href="https://leandeep.com/about/">About</a></li><li><a href="https://leandeep.com/finance/">Finance</a></li><li><a href="https://leandeep.com/notebooks/">ML Notebooks</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>

            <div class="content">
                
    <main class="post">

        <div class="post-info">
            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>2 minutes

            

            </p>
        </div>

        <article>
            <h1 class="post-title">
                <a href="https://leandeep.com/utiliser-ollama-sur-ubuntu-22-via-docker-et-faire-du-llm-scraping/">Utiliser Ollama sur Ubuntu 22 via Docker et faire du LLM scraping</a>
            </h1>

            

            <div class="post-content">
                <p>Dans cet article, nous allons voir comment utiliser Ollama sur Ubuntu 22 via Docker tout en tirant parti du GPU connect√© au serveur dans le but de faire du scraping en posant des questions √† son LLM auto-h√©berg√©.</p>
<br/>
<p><strong>Pr√©-requis</strong></p>
<ul>
<li><code>nvidia-smi</code> d√©j√† install√©</li>
<li>Cuda install√©</li>
<li>Docker install√©</li>
</ul>
<br/>
<p><strong>Installation du NVIDIA Container Toolkit</strong></p>
<pre tabindex="0"><code>sudo apt install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
# V√©rification
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu20.04 nvidia-smi
</code></pre><br/>
<p><strong>Docker compose Ollama avec acc√®s GPU</strong></p>
<pre tabindex="0"><code>version: &#39;3.8&#39;
services:
  ollama:
    image: ollama/ollama
    ports:
      - &#34;11434:11434&#34;
    volumes:
      - ollama-data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    ports:
      - &#34;3000:8080&#34;
    volumes:
      - openwebui-data:/app/backend/data
    environment:
      - OLLAMA_API_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped
  
  scraper-llm:
    build:
      context: ./scraper-llm
    depends_on:
      - ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]

volumes:
  ollama-data:
  openwebui-data:
</code></pre><br/>
<p><strong>Ajouter les 3 fichiers ci-dessous permettant de scraper des sites web</strong></p>
<ol>
<li>Cr√©er un fichier <code>./scraper-llm/main.py</code> et ajouter le contenu suivant:</li>
</ol>
<pre tabindex="0"><code>import requests
from bs4 import BeautifulSoup
import html2text

OLLAMA_URL = &#34;http://ollama:11434/api/generate&#34;  # utilise le nom du service docker

def scrape_text(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, &#39;html.parser&#39;)
    html_text = str(soup.body)
    return html2text.html2text(html_text)

def ask_ollama(prompt, model=&#34;llama3&#34;):
    payload = {
        &#34;model&#34;: model,
        &#34;prompt&#34;: prompt,
        &#34;stream&#34;: False
    }
    response = requests.post(OLLAMA_URL, json=payload)
    response.raise_for_status()
    return response.json()[&#34;response&#34;]

if __name__ == &#34;__main__&#34;:
    url = &#34;https://fr.wikipedia.org/wiki/Intelligence_artificielle&#34;
    content = scrape_text(url)

    question = &#34;Quels sont les grands types d&#39;intelligence artificielle abord√©s dans cet article ?&#34;
    full_prompt = f&#34;Voici le contenu d&#39;une page web :\n\n{content}\n\n{question}&#34;

    print(&#34;üß† Question envoy√©e au mod√®le...\n&#34;)
    answer = ask_ollama(full_prompt)
    print(&#34;‚úÖ R√©ponse :\n&#34;)
    print(answer)
</code></pre><br/>
<ol start="2">
<li>Cr√©er le fichier <code>./scraper-llm/requirements.txt</code> et ajouter le contenu suivant:</li>
</ol>
<pre tabindex="0"><code>requests
beautifulsoup4
html2text
</code></pre><br/>
<ol start="3">
<li>Cr√©er un <code>./scraper-llm/Dockerfile</code>:</li>
</ol>
<pre tabindex="0"><code>FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY main.py .

CMD [&#34;python&#34;, &#34;main.py&#34;]
</code></pre><br/>
<h2 id="run">RUN</h2>
<pre tabindex="0"><code>docker compose build --no-cache
docker compose up

# Dans une second tab:
docker compose exec ollama ollama run llama3
docker compose exec ollama ollama run llama4
</code></pre><br/>
<ul>
<li>Web Interface: http://localhost:3000</li>
<li>Ollama API: http://localhost:11434</li>
</ul>

            </div>
        </article>

        <hr />

        <div class="post-info">
                <p>
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="https://leandeep.com/tags/linux">Linux</a></span><span class="tag"><a href="https://leandeep.com/tags/llm">LLM</a></span><span class="tag"><a href="https://leandeep.com/tags/ai">AI</a></span>
                </p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>326 Mots</p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>04 mai. 2025</p>
        </div>

        
            <div class="pagination">
                <div class="pagination__title">
                    <span class="pagination__title-h"></span>
                    <hr />
                </div>

                <div class="pagination__buttons">
                    
                        <span class="button previous">
                            <a href="https://leandeep.com/install-nostr-rust-relay-without-docker/">
                                <span class="button__icon">‚Üê</span>
                                <span class="button__text">Install Nostr Rust relay without Docker</span>
                            </a>
                        </span>
                    

                    
                        <span class="button next">
                            <a href="https://leandeep.com/identify-path-to-stop-docker-compose/">
                                <span class="button__text">Identify path to stop docker compose</span>
                                <span class="button__icon">‚Üí</span>
                            </a>
                        </span>
                    
                </div>
            </div>
        

        
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>Built by <a href="https://www.linkedin.com/in/oliviereeckhoutte/">Olivier Eeckhoutte</a>,
                Freelance @ LeanDeep <a href="https://leandeep.com/about/">(üçÉ company)</a></span>
            <span>Siret: 83825337500011</span>
            <span><a href="https://leandeep.com/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss">
                        <path d="M4 11a9 9 0 0 1 9 9"></path>
                        <path d="M4 4a16 16 0 0 1 16 16"></path>
                        <circle cx="5" cy="19" r="1"></circle>
                    </svg></a></span>
        </div>
    </div>
</footer>
            
        </div>

        




<script type="text/javascript" src="https://leandeep.com/bundle.min.c184f8481b5847ad1a7d8aa775944fa063f118cb4df68f4eaa3826a2a2e16b26a1ad798f5160210f265c6fbb9a5f19b953fed066ae1ed1092d1858bcff13ae92.js" integrity="sha512-wYT4SBtYR60afYqndZRPoGPxGMtN9o9OqjgmoqLhayahrXmPUWAhDyZcb7uaXxm5U/7QZq4e0QktGFi8/xOukg=="></script>







    </body>
</html>
