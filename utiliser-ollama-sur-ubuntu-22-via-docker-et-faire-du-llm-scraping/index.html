<!DOCTYPE html>
<html lang="fr">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author"
  content="Olivier Eeckhoutte">
<meta name="description"
  content="Dans cet article, nous allons voir comment utiliser Ollama sur Ubuntu 22 via Docker tout en tirant parti du GPU connecté au serveur dans le but de faire du scraping en posant des questions à son LLM auto-hébergé.
Pré-requis
nvidia-smi déjà installé Cuda installé Docker installé Installation du NVIDIA Container Toolkit
sudo apt install -y nvidia-container-toolkit sudo nvidia-ctk runtime configure --runtime=docker sudo systemctl restart docker # Vérification docker run --rm --gpus all nvidia/cuda:12." />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<script>
  
  
  if (!(window.location.host.startsWith("127.0.0.1")) && !(window.location.host.startsWith("localhost"))) {
    if (window.location.protocol != "https:") {
      console.log("Redirecting to https...")
      window.location.protocol = "https";
    }
  }
</script>


<link rel="canonical" href="https://leandeep.com/utiliser-ollama-sur-ubuntu-22-via-docker-et-faire-du-llm-scraping/" />



<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

<style>
  #app {
    display: none;
    border-radius: 10px;
    box-shadow: 2px 5px 12px -1px rgba(0, 0, 0, 0.56);
    padding: 20px;
    background-color: white;
    max-width: 500px;
    margin: 15px auto;
    text-align: center;
    min-height: 500px;
  }

  #app input {
    margin: 0 auto;
    float: none;
    width: 100%;
    max-width: 300px;
    padding: 5px 10px;
    border: 2px solid black;
  }

  #app ul {
    margin: 0;
    padding: 0;
  }

  #app li {
    text-align: left;
    padding: 5px 10px;
    width: 100%;
    max-width: 280px;
    margin: 1px auto;
    background-color: white;
    border: 1px solid black;
    list-style: none;
  }
</style>




<title>
  
  Utiliser Ollama sur Ubuntu 22 via Docker et faire du LLM scraping :: Lean Deep Tech blog 
  
</title>



<link href="//cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
  type="text/css">



<link rel="stylesheet" href="https://leandeep.com/main.min.44eaa49e743eabd51724579f1d2ece0dac6f56215301d6961ca74092199d4a05.css">




    <link rel="apple-touch-icon" sizes="180x180" href="https://leandeep.com/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://leandeep.com/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://leandeep.com/favicon-16x16.png">
    <link rel="manifest" href="https://leandeep.com/site.webmanifest">
    <link rel="mask-icon" href="https://leandeep.com/safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="https://leandeep.com/favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">

<meta itemprop="name" content="Utiliser Ollama sur Ubuntu 22 via Docker et faire du LLM scraping">
<meta itemprop="description" content="Dans cet article, nous allons voir comment utiliser Ollama sur Ubuntu 22 via Docker tout en tirant parti du GPU connecté au serveur dans le but de faire du scraping en posant des questions à son LLM auto-hébergé.
Pré-requis
nvidia-smi déjà installé Cuda installé Docker installé Installation du NVIDIA Container Toolkit
sudo apt install -y nvidia-container-toolkit sudo nvidia-ctk runtime configure --runtime=docker sudo systemctl restart docker # Vérification docker run --rm --gpus all nvidia/cuda:12."><meta itemprop="datePublished" content="2025-05-04T23:32:00+02:00" />
<meta itemprop="dateModified" content="2025-05-04T23:32:00+02:00" />
<meta itemprop="wordCount" content="326"><meta itemprop="image" content="https://leandeep.com"/>
<meta itemprop="keywords" content="Linux,LLM,AI," /><meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://leandeep.com"/>

<meta name="twitter:title" content="Utiliser Ollama sur Ubuntu 22 via Docker et faire du LLM scraping"/>
<meta name="twitter:description" content="Dans cet article, nous allons voir comment utiliser Ollama sur Ubuntu 22 via Docker tout en tirant parti du GPU connecté au serveur dans le but de faire du scraping en posant des questions à son LLM auto-hébergé.
Pré-requis
nvidia-smi déjà installé Cuda installé Docker installé Installation du NVIDIA Container Toolkit
sudo apt install -y nvidia-container-toolkit sudo nvidia-ctk runtime configure --runtime=docker sudo systemctl restart docker # Vérification docker run --rm --gpus all nvidia/cuda:12."/>





<meta property="article:published_time" content="2025-05-04 23:32:00 &#43;0200 CEST" />







    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://leandeep.com/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">$ cd /home/leandeep</span>
            <span class="logo__cursor" style=""></span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://leandeep.com/events/">Featured Articles</a></li><li><a href="https://leandeep.com/posts/">All Articles</a></li><li><a href="https://leandeep.com/about/">About</a></li><li><a href="https://leandeep.com/finance/">Finance</a></li><li><a href="https://leandeep.com/notebooks/">ML Notebooks</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>

            <div class="content">
                
    <main class="post">

        <div class="post-info">
            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>2 minutes

            

            </p>
        </div>

        <article>
            <h1 class="post-title">
                <a href="https://leandeep.com/utiliser-ollama-sur-ubuntu-22-via-docker-et-faire-du-llm-scraping/">Utiliser Ollama sur Ubuntu 22 via Docker et faire du LLM scraping</a>
            </h1>

            

            <div class="post-content">
                <p>Dans cet article, nous allons voir comment utiliser Ollama sur Ubuntu 22 via Docker tout en tirant parti du GPU connecté au serveur dans le but de faire du scraping en posant des questions à son LLM auto-hébergé.</p>
<br/>
<p><strong>Pré-requis</strong></p>
<ul>
<li><code>nvidia-smi</code> déjà installé</li>
<li>Cuda installé</li>
<li>Docker installé</li>
</ul>
<br/>
<p><strong>Installation du NVIDIA Container Toolkit</strong></p>
<pre tabindex="0"><code>sudo apt install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
# Vérification
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu20.04 nvidia-smi
</code></pre><br/>
<p><strong>Docker compose Ollama avec accès GPU</strong></p>
<pre tabindex="0"><code>version: &#39;3.8&#39;
services:
  ollama:
    image: ollama/ollama
    ports:
      - &#34;11434:11434&#34;
    volumes:
      - ollama-data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    ports:
      - &#34;3000:8080&#34;
    volumes:
      - openwebui-data:/app/backend/data
    environment:
      - OLLAMA_API_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    restart: unless-stopped
  
  scraper-llm:
    build:
      context: ./scraper-llm
    depends_on:
      - ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]

volumes:
  ollama-data:
  openwebui-data:
</code></pre><br/>
<p><strong>Ajouter les 3 fichiers ci-dessous permettant de scraper des sites web</strong></p>
<ol>
<li>Créer un fichier <code>./scraper-llm/main.py</code> et ajouter le contenu suivant:</li>
</ol>
<pre tabindex="0"><code>import requests
from bs4 import BeautifulSoup
import html2text

OLLAMA_URL = &#34;http://ollama:11434/api/generate&#34;  # utilise le nom du service docker

def scrape_text(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, &#39;html.parser&#39;)
    html_text = str(soup.body)
    return html2text.html2text(html_text)

def ask_ollama(prompt, model=&#34;llama3&#34;):
    payload = {
        &#34;model&#34;: model,
        &#34;prompt&#34;: prompt,
        &#34;stream&#34;: False
    }
    response = requests.post(OLLAMA_URL, json=payload)
    response.raise_for_status()
    return response.json()[&#34;response&#34;]

if __name__ == &#34;__main__&#34;:
    url = &#34;https://fr.wikipedia.org/wiki/Intelligence_artificielle&#34;
    content = scrape_text(url)

    question = &#34;Quels sont les grands types d&#39;intelligence artificielle abordés dans cet article ?&#34;
    full_prompt = f&#34;Voici le contenu d&#39;une page web :\n\n{content}\n\n{question}&#34;

    print(&#34;🧠 Question envoyée au modèle...\n&#34;)
    answer = ask_ollama(full_prompt)
    print(&#34;✅ Réponse :\n&#34;)
    print(answer)
</code></pre><br/>
<ol start="2">
<li>Créer le fichier <code>./scraper-llm/requirements.txt</code> et ajouter le contenu suivant:</li>
</ol>
<pre tabindex="0"><code>requests
beautifulsoup4
html2text
</code></pre><br/>
<ol start="3">
<li>Créer un <code>./scraper-llm/Dockerfile</code>:</li>
</ol>
<pre tabindex="0"><code>FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY main.py .

CMD [&#34;python&#34;, &#34;main.py&#34;]
</code></pre><br/>
<h2 id="run">RUN</h2>
<pre tabindex="0"><code>docker compose build --no-cache
docker compose up

# Dans une second tab:
docker compose exec ollama ollama run llama3
docker compose exec ollama ollama run llama4
</code></pre><br/>
<ul>
<li>Web Interface: http://localhost:3000</li>
<li>Ollama API: http://localhost:11434</li>
</ul>

            </div>
        </article>

        <hr />

        <div class="post-info">
                <p>
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="https://leandeep.com/tags/linux">Linux</a></span><span class="tag"><a href="https://leandeep.com/tags/llm">LLM</a></span><span class="tag"><a href="https://leandeep.com/tags/ai">AI</a></span>
                </p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>326 Mots</p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>04 mai. 2025</p>
        </div>

        
            <div class="pagination">
                <div class="pagination__title">
                    <span class="pagination__title-h"></span>
                    <hr />
                </div>

                <div class="pagination__buttons">
                    
                        <span class="button previous">
                            <a href="https://leandeep.com/install-nostr-rust-relay-without-docker/">
                                <span class="button__icon">←</span>
                                <span class="button__text">Install Nostr Rust relay without Docker</span>
                            </a>
                        </span>
                    

                    
                        <span class="button next">
                            <a href="https://leandeep.com/identify-path-to-stop-docker-compose/">
                                <span class="button__text">Identify path to stop docker compose</span>
                                <span class="button__icon">→</span>
                            </a>
                        </span>
                    
                </div>
            </div>
        

        
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>Built by <a href="https://www.linkedin.com/in/oliviereeckhoutte/">Olivier Eeckhoutte</a>,
                Freelance @ LeanDeep <a href="https://leandeep.com/about/">(🍃 company)</a></span>
            <span>Siret: 83825337500011</span>
            <span><a href="https://leandeep.com/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss">
                        <path d="M4 11a9 9 0 0 1 9 9"></path>
                        <path d="M4 4a16 16 0 0 1 16 16"></path>
                        <circle cx="5" cy="19" r="1"></circle>
                    </svg></a></span>
        </div>
    </div>
</footer>
            
        </div>

        




<script type="text/javascript" src="https://leandeep.com/bundle.min.c184f8481b5847ad1a7d8aa775944fa063f118cb4df68f4eaa3826a2a2e16b26a1ad798f5160210f265c6fbb9a5f19b953fed066ae1ed1092d1858bcff13ae92.js" integrity="sha512-wYT4SBtYR60afYqndZRPoGPxGMtN9o9OqjgmoqLhayahrXmPUWAhDyZcb7uaXxm5U/7QZq4e0QktGFi8/xOukg=="></script>







    </body>
</html>
