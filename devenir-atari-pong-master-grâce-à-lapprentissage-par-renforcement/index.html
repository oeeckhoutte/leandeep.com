<!DOCTYPE html>
<html lang="fr">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author"
  content="Olivier Eeckhoutte">
<meta name="description"
  content="Qu&amp;rsquo;est-ce que l&amp;rsquo;apprentissage par renforcement ? L&amp;rsquo;apprentissage par renforcement est utilisé dans l&amp;rsquo;intelligence artificielle pour enseigner aux ordinateurs comment prendre de meilleures décisions en fonction des récompenses qu&amp;rsquo;ils reçoivent.
En d&amp;rsquo;autres termes, l&amp;rsquo;apprentissage par renforcement, c&amp;rsquo;est un peu comme apprendre à faire quelque chose en obtenant des récompenses pour ses actions.
En trading par exemple, on évalue quelle stratégie va maximiser les récompenses qui sont le retour sur investissement. Les récompenses peuvent être obtenues longtemps après une action.
Autre exemple, avec un jeu d&amp;rsquo;échec, on peut obtenir des récompenses mieux que ce qu&amp;rsquo;on aurait pu jouer simplement en sacrifiant des pièces pour jouer un meilleur coup.
En apprentissage par renforcement, on crée une politique qui définit l&amp;rsquo;action qui maximisera les récompenses lorsqu&amp;rsquo;une action sera exécutée en fonction de l&amp;rsquo;état du système.
Atari Pong Pour coder mon réseau de neurones et faire de l&amp;rsquo;apprentissage par renforcement sur un cas pratique et simple, j&amp;rsquo;ai utilisé le framework OpenAI gym.
&amp;ldquo;Gym is a toolkit for developing and comparing reinforcement learning algorithms&amp;rdquo;
Ce framework permet d&amp;rsquo;intéragir avec des jeux basiques Atari. J&amp;rsquo;ai choisi le jeu Atari Pong pour implémenter mon algorithme gradients de politique.
Voici 2 vidéos que j&amp;rsquo;ai enregistré qui montrent des parties jouées entre un agent qui est l&amp;rsquo;ordinateur et un agent qui est piloté par un réseau de neurones.
Dans la première vidéo, le réseau de neurones n&amp;rsquo;a pas encore été entraîné. Dans la seconde vidéo, le réseau de neurones a été entraîné pendant des jours avec un algorithme policy gradients. " />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<script>
  
  
  if (!(window.location.host.startsWith("127.0.0.1")) && !(window.location.host.startsWith("localhost"))) {
    if (window.location.protocol != "https:") {
      console.log("Redirecting to https...")
      window.location.protocol = "https";
    }
  }
</script>


<link rel="canonical" href="https://leandeep.com/devenir-atari-pong-master-gr%C3%A2ce-%C3%A0-lapprentissage-par-renforcement/" />




<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">



<script src="https://fb.me/react-15.1.0.min.js"></script>
<script src="https://fb.me/react-dom-15.1.0.min.js"></script>
<style>
  .nav-search {
    display: none;
    -webkit-flex-grow: 1;
    -ms-flex-positive: 1;
    position: relative;
    width: 90%;
    height: 47px;
    margin-top: 20px;
    background-color: white;
    z-index: 1000;
  }

  .nav-search.active {
    box-shadow: 0 4px 4px rgba(79, 79, 79, 0.21);
  }

  .nav-search.active .search-dropdown {
    display: block;
  }

  .nav-search.active .search-input {
    -webkit-animation: expand-search-box-animation 0.5s forwards;
    animation: expand-search-box-animation 0.5s forwards;
  }

  .nav-search.active .search-input input {
    border-width: 2px;
  }

  .nav-search.active .search-input .close-search {
    display: inline-block;
  }

  .nav-search.active .search-input .search-dropdown {
    display: block;
  }

  .nav-search .search-input {
    transition: left 0.2s ease-in-out;
    transition: width 0s ease-in-out;
  }

  .nav-search .search-input .search-icon {
    position: absolute;
    left: 15px;
    top: 13px;
    z-index: 999;
    color: black;
  }

  .nav-search .search-input input {
    font: 16px/1.875 "Avenir Next W01", "Avenir Next", "Helvetica Neue", Helvetica, sans-serif;
    height: 50px;
    border: 1px solid #1b98f4;
    border-radius: 4px;
    min-width: 200px;
    width: 100%;
    padding-left: 50px;
    background-color: white;
  }

  .nav-search .search-input input:focus {
    outline: none;
  }

  .nav-search .search-input i.close-search {
    color: #1b98f4;
    display: none;
    position: absolute;
    right: 15px;
    top: 13px;
    cursor: pointer;
  }

  .search-dropdown {
    box-sizing: border-box;
    color: #B3B3B3;
    font: 14px/1.875 "Avenir Next W01", "Avenir Next", "Helvetica Neue", Helvetica, sans-serif;
    opacity: 1.00;
    padding: 20px;
    width: 100%;
    -webkit-animation: expand-search-dropdown-animation 0.5s forwards;
    animation: expand-search-dropdown-animation 0.5s forwards;
    overflow-y: scroll;
    max-height: 400px;
    border-radius: 0 0 4px 4px;
    background-color: #FCFCFC;
    border: 1px solid #E0E0E0;
    box-shadow: 1px 3px 4px rgba(0, 0, 0, 0.09);
    display: none;
    background-color: white;
  }

  .search-dropdown .small {
    -webkit-flex-basis: 35%;
    -ms-flex-preferred-size: 35%;
    flex-basis: 35%;
  }

  .search-dropdown .search-section .hits-blank {
    color: #666;
    text-align: center;
    padding-top: 20px;
  }

  .search-dropdown a {
    text-decoration: none;
    color: inherit;
    z-index: 2000;
  }

  .hit {
    border-bottom: 1px solid #E6E6E6;
    margin-bottom: 20px;
  }

  .hit .hit-title {
    color: #1b98f4;
    font-family: 'bt_mono', monospace;
    font-weight: 500;
    margin-bottom: 0;
    margin-top: 0;
    display: inline-block;
    font-size: 14px;
  }

  .hit .hit-description {
    text-decoration: none;
    color: black;
    font-size: 14px;
    display: block;
    margin-top: 3px;
  }

  .hit .hit-anchor {
    font-size: 13px;
    color: #666;
  }

  .hit .algolia-docsearch-suggestion--highlight {
    background-color: #FFE9A4;
  }

  .hit:last-child {
     
  }

  .ais-hits--item:last-child .hit {
    border: 0;
  }
</style>

<style>
  #app {
    display: none;
     
    border-radius: 10px;
    box-shadow: 2px 5px 12px -1px rgba(0, 0, 0, 0.56);
    padding: 20px;
    background-color: white;
    max-width: 500px;
    margin: 15px auto;
    text-align: center;
    min-height: 500px;
  }

  #app input {
    margin: 0 auto;
    float: none;
    width: 100%;
    max-width: 300px;
    padding: 5px 10px;
    border: 2px solid black;
  }

  #app ul {
    margin: 0;
    padding: 0;
  }

  #app li {
    text-align: left;
    padding: 5px 10px;
    width: 100%;
    max-width: 280px;
    margin: 1px auto;
    background-color: white;
    border: 1px solid black;
    list-style: none;
  }
</style>




<title>
  
  Devenir Atari Pong master grâce à l&#39;apprentissage par renforcement :: Lean Deep Tech blog 
  
</title>



<link href="//cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
  type="text/css">



<link rel="stylesheet" href="https://leandeep.com/main.min.5a145ada8612b7e3e366691ddad4ba3ce6007aa2562c494a29f0b76b636f9789.css">




    <link rel="apple-touch-icon" sizes="180x180" href="https://leandeep.com/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://leandeep.com/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://leandeep.com/favicon-16x16.png">
    <link rel="manifest" href="https://leandeep.com/site.webmanifest">
    <link rel="mask-icon" href="https://leandeep.com/safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="https://leandeep.com/favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">

<meta itemprop="name" content="Devenir Atari Pong master grâce à l&#39;apprentissage par renforcement">
<meta itemprop="description" content="Qu&rsquo;est-ce que l&rsquo;apprentissage par renforcement ?
L&rsquo;apprentissage par renforcement est utilisé dans l&rsquo;intelligence artificielle pour enseigner aux ordinateurs comment prendre de meilleures décisions en fonction des récompenses qu&rsquo;ils reçoivent.
En d&rsquo;autres termes, l&rsquo;apprentissage par renforcement, c&rsquo;est un peu comme apprendre à faire quelque chose en obtenant des récompenses pour ses actions.
En trading par exemple, on évalue quelle stratégie va maximiser les récompenses qui sont le retour sur investissement. Les récompenses peuvent être obtenues longtemps après une action.
Autre exemple, avec un jeu d&rsquo;échec, on peut obtenir des récompenses mieux que ce qu&rsquo;on aurait pu jouer simplement en sacrifiant des pièces pour jouer un meilleur coup.

En apprentissage par renforcement, on crée une politique qui définit l&rsquo;action qui maximisera les récompenses lorsqu&rsquo;une action sera exécutée en fonction de l&rsquo;état du système.


Atari Pong
Pour coder mon réseau de neurones et faire de l&rsquo;apprentissage par renforcement sur un cas pratique et simple, j&rsquo;ai utilisé le framework OpenAI gym.

&ldquo;Gym is a toolkit for developing and comparing reinforcement learning algorithms&rdquo;

Ce framework permet d&rsquo;intéragir avec des jeux basiques Atari.
J&rsquo;ai choisi le jeu Atari Pong pour implémenter mon algorithme gradients de politique.
Voici 2 vidéos que j&rsquo;ai enregistré qui montrent des parties jouées entre un agent qui est l&rsquo;ordinateur et un agent qui est piloté par un réseau de neurones.

Dans la première vidéo, le réseau de neurones n&rsquo;a pas encore été entraîné.


    
    







Dans la seconde vidéo, le réseau de neurones a été entraîné pendant des jours avec un algorithme policy gradients.


    
    




"><meta itemprop="datePublished" content="2018-03-29T21:09:00+00:00" />
<meta itemprop="dateModified" content="2018-03-29T21:09:00+00:00" />
<meta itemprop="wordCount" content="846"><meta itemprop="image" content="https://leandeep.com"/>
<meta itemprop="keywords" content="Featured,Apprentissage Renforcement,Machine Learning," /><meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://leandeep.com"/>

<meta name="twitter:title" content="Devenir Atari Pong master grâce à l&#39;apprentissage par renforcement"/>
<meta name="twitter:description" content="Qu&rsquo;est-ce que l&rsquo;apprentissage par renforcement ?
L&rsquo;apprentissage par renforcement est utilisé dans l&rsquo;intelligence artificielle pour enseigner aux ordinateurs comment prendre de meilleures décisions en fonction des récompenses qu&rsquo;ils reçoivent.
En d&rsquo;autres termes, l&rsquo;apprentissage par renforcement, c&rsquo;est un peu comme apprendre à faire quelque chose en obtenant des récompenses pour ses actions.
En trading par exemple, on évalue quelle stratégie va maximiser les récompenses qui sont le retour sur investissement. Les récompenses peuvent être obtenues longtemps après une action.
Autre exemple, avec un jeu d&rsquo;échec, on peut obtenir des récompenses mieux que ce qu&rsquo;on aurait pu jouer simplement en sacrifiant des pièces pour jouer un meilleur coup.

En apprentissage par renforcement, on crée une politique qui définit l&rsquo;action qui maximisera les récompenses lorsqu&rsquo;une action sera exécutée en fonction de l&rsquo;état du système.


Atari Pong
Pour coder mon réseau de neurones et faire de l&rsquo;apprentissage par renforcement sur un cas pratique et simple, j&rsquo;ai utilisé le framework OpenAI gym.

&ldquo;Gym is a toolkit for developing and comparing reinforcement learning algorithms&rdquo;

Ce framework permet d&rsquo;intéragir avec des jeux basiques Atari.
J&rsquo;ai choisi le jeu Atari Pong pour implémenter mon algorithme gradients de politique.
Voici 2 vidéos que j&rsquo;ai enregistré qui montrent des parties jouées entre un agent qui est l&rsquo;ordinateur et un agent qui est piloté par un réseau de neurones.

Dans la première vidéo, le réseau de neurones n&rsquo;a pas encore été entraîné.


    
    







Dans la seconde vidéo, le réseau de neurones a été entraîné pendant des jours avec un algorithme policy gradients.


    
    




"/>





<meta property="article:published_time" content="2018-03-29 21:09:00 &#43;0000 UTC" />







    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://leandeep.com/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">$ cd /home/leandeep</span>
            <span class="logo__cursor" style=""></span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://leandeep.com/events/">Featured Articles</a></li><li><a href="https://leandeep.com/posts/">All Articles</a></li><li><a href="https://leandeep.com/about/">About</a></li><li><a href="https://leandeep.com/finance/">Finance</a></li><li><a href="https://leandeep.com/notebooks/">ML Notebooks</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>

            <div class="content">
                
    <main class="post">

        <div class="post-info">
            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>4 minutes

            

            </p>
        </div>

        <article>
            <h1 class="post-title">
                <a href="https://leandeep.com/devenir-atari-pong-master-gr%C3%A2ce-%C3%A0-lapprentissage-par-renforcement/">Devenir Atari Pong master grâce à l&rsquo;apprentissage par renforcement</a>
            </h1>

            

            <div class="post-content">
                <h2 id="quest-ce-que-lapprentissage-par-renforcement-">Qu&rsquo;est-ce que l&rsquo;apprentissage par renforcement ?</h2>
<p>L&rsquo;apprentissage par renforcement est utilisé dans l&rsquo;intelligence artificielle pour enseigner aux ordinateurs comment prendre de meilleures décisions en fonction des récompenses qu&rsquo;ils reçoivent.</p>
<p>En d&rsquo;autres termes, l&rsquo;apprentissage par renforcement, c&rsquo;est un peu comme apprendre à faire quelque chose en obtenant des récompenses pour ses actions.</p>
<p>En trading par exemple, on évalue quelle stratégie va maximiser les récompenses qui sont le retour sur investissement. Les récompenses peuvent être obtenues longtemps après une action.</p>
<p>Autre exemple, avec un jeu d&rsquo;échec, on peut obtenir des récompenses mieux que ce qu&rsquo;on aurait pu jouer simplement en sacrifiant des pièces pour jouer un meilleur coup.</p>
<blockquote>
<p>En apprentissage par renforcement, on crée une <strong>politique</strong> qui définit l&rsquo;<strong>action</strong> qui maximisera les <strong>récompenses</strong> lorsqu&rsquo;une action sera exécutée en fonction de l&rsquo;état du système.</p>
</blockquote>
<br/>
<h2 id="atari-pong">Atari Pong</h2>
<p>Pour coder mon réseau de neurones et faire de l&rsquo;apprentissage par renforcement sur un cas pratique et simple, j&rsquo;ai utilisé le framework <a href="https://gym.openai.com/docs/">OpenAI gym</a>.</p>
<blockquote>
<p>&ldquo;Gym is a toolkit for developing and comparing reinforcement learning algorithms&rdquo;</p>
</blockquote>
<p>Ce framework permet d&rsquo;intéragir avec des jeux basiques Atari.
J&rsquo;ai choisi le jeu Atari Pong pour implémenter mon algorithme gradients de politique.</p>
<p>Voici 2 vidéos que j&rsquo;ai enregistré qui montrent des parties jouées entre un agent qui est l&rsquo;ordinateur et un agent qui est piloté par un réseau de neurones.</p>
<ul>
<li>Dans la première vidéo, le réseau de neurones n&rsquo;a pas encore été entraîné.</li>
</ul>

    <iframe 
        width="100%" 
        height="400" 
        src="//www.youtube.com/embed/fmxJNCf4REY?rel=0" 
        frameborder="0" 
        allow="autoplay; encrypted-media" 
        allowfullscreen>
    </iframe>




<p><br/>
<br/></p>
<ul>
<li>Dans la seconde vidéo, le réseau de neurones a été entraîné pendant des jours avec un algorithme policy gradients.</li>
</ul>

    <iframe 
        width="100%" 
        height="400" 
        src="//www.youtube.com/embed/yo2c-SCFL-M?rel=0" 
        frameborder="0" 
        allow="autoplay; encrypted-media" 
        allowfullscreen>
    </iframe>




<!-- raw HTML omitted -->
<br/>
<br/>
<h2 id="algorithmes-gradients-de-politiquenbsppolicy-gradients">Algorithmes gradients de politique <em>(Policy Gradients)</em></h2>
<p>Les algorithmes de gradient de politique sont un type d&rsquo;algorithme d&rsquo;apprentissage par renforcement qui utilisent la descente de gradient pour apprendre une politique qui maximise une récompense donnée.</p>
<p>Les algorithmes gradients de politique sont utilisés pour apprendre à un ordinateur comment prendre de meilleures décisions pour accomplir une tâche spécifique. Voici comment ça marche :</p>
<ul>
<li>
<p>Objectif: Tout d&rsquo;abord, l&rsquo;ordinateur a une tâche à accomplir, comme jouer à un jeu ou piloter un robot.</p>
</li>
<li>
<p>Actions: Il peut effectuer différentes actions pour essayer de bien faire cette tâche. Par exemple, dans un jeu vidéo, il peut décider de bouger vers la gauche ou vers la droite.</p>
</li>
<li>
<p>Politique: L&rsquo;ordinateur a une sorte de plan, appelé &ldquo;politique&rdquo;, qui lui dit quelles actions prendre dans différentes situations. Au début, ce plan peut être aléatoire, c&rsquo;est-à-dire qu&rsquo;il fait des choses au hasard.</p>
</li>
<li>
<p>Récompense: À chaque fois qu&rsquo;il fait quelque chose, il reçoit une récompense. Si ce qu&rsquo;il fait est bon, la récompense est élevée ; s&rsquo;il se trompe, la récompense est faible.</p>
</li>
<li>
<p>Apprentissage: Maintenant, l&rsquo;ordinateur veut devenir meilleur. Il regarde les récompenses qu&rsquo;il a reçues et ajuste sa politique. Si une action a conduit à une grande récompense, il est plus susceptible de faire cette action dans le futur. Si une action a conduit à une petite récompense, il est moins susceptible de la faire à l&rsquo;avenir.</p>
</li>
<li>
<p>Répétition: Il continue à jouer ou à effectuer des actions, à recevoir des récompenses, à ajuster sa politique, encore et encore, jusqu&rsquo;à ce qu&rsquo;il devienne vraiment bon à la tâche.</p>
</li>
</ul>
<p>Donc, les algorithmes gradients de politique sont comme un apprentissage par essais et erreurs, où l&rsquo;ordinateur apprend à faire de meilleures actions en se basant sur les récompenses qu&rsquo;il reçoit. C&rsquo;est un peu comme quand tu essaies de faire quelque chose, comme dessiner, et que tu t&rsquo;améliores en pratiquant et en apprenant de tes erreurs.</p>
<br/>
<p>Voici un très bon cours pour comprendre en détail comment fonctionnent ces algorithmes:
<a href="http://rll.berkeley.edu/deeprlcourse/">http://rll.berkeley.edu/deeprlcourse/</a> (voir lecture 4 policy gradient).</p>
<p>Il y a également cette vidéo qui traite spécifiquement de ces algorithmes:</p>

    <iframe 
        width="100%" 
        height="400" 
        src="//www.youtube.com/embed/oPGVsoBonLM?rel=0" 
        frameborder="0" 
        allow="autoplay; encrypted-media" 
        allowfullscreen>
    </iframe>




<p>En synthèse, voici un résumé de l&rsquo;algo implémenté:</p>
<ul>
<li>
<p>On commence par laisser la politique du réseau de neurones jouer plusieurs parties de Pong et à chaque étape on calcule les gradients qui augmenteraient la probabilité de l&rsquo;action choisie. Au début, on ne fait rien d&rsquo;autre que calculer les gradients.</p>
</li>
<li>
<p>Après plusieurs parties, on calcule le score de chaque action. On évalue une action en fonction de la somme de toutes les récompenses qui s&rsquo;ensuivent en appliquant à chaque étape un taux de rabais (<strong>discount rate</strong>) r.
Par exemple, si on considère que notre agent va 3 fois de suite vers le haut et qu&rsquo;on prend les hypothèses suivantes:</p>
<ul>
<li>Il reçoit +10 comme récompense après la première étape</li>
<li>0 après la deuxième</li>
<li>-50 après la troisième</li>
<li>r = 0,8 (plus le taux de rabais est proche de 1 et plus les récompenses arrivant tardivement comptent autant que les récompenses immédiates. Généralemement r varie entre 0,95 et 0,99.</li>
<li>Conclusion: le score total est de 10 * r^0 + 0 * r^1 + (-50) * r^2 = -22</li>
</ul>
</li>
<li>
<p>Si le score est positif, cela signifie que l&rsquo;action était bonne. On applique les gradients calculés lors de la première étape. Appliquer les gradients signifie que l&rsquo;on multiplie son vecteur par le score de l&rsquo;action correspondante. L&rsquo;action sera ainsi davantage utilisée dans le futur.
Si l&rsquo;action n&rsquo;était pas bonne, on applique les gradients opposés.</p>
</li>
<li>
<p>Pour terminer, on calcule la moyenne de tous les vecteurs de gradients obtenus et on l&rsquo;utilise pour effectuer une étape de descente de gradient.</p>
</li>
</ul>
            </div>
        </article>

        <hr />

        <div class="post-info">
                <p>
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="https://leandeep.com/tags/featured">Featured</a></span><span class="tag"><a href="https://leandeep.com/tags/apprentissage-renforcement">Apprentissage Renforcement</a></span><span class="tag"><a href="https://leandeep.com/tags/machine-learning">Machine Learning</a></span>
                </p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>846 Mots</p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>29 mars. 2018</p>
        </div>

        
            <div class="pagination">
                <div class="pagination__title">
                    <span class="pagination__title-h"></span>
                    <hr />
                </div>

                <div class="pagination__buttons">
                    
                        <span class="button previous">
                            <a href="https://leandeep.com/pytest-tricks/">
                                <span class="button__icon">←</span>
                                <span class="button__text">Pytest tricks</span>
                            </a>
                        </span>
                    

                    
                        <span class="button next">
                            <a href="https://leandeep.com/voiture-autonome-avec-unity-et-keras/">
                                <span class="button__text">Voiture autonome avec Unity et Keras</span>
                                <span class="button__icon">→</span>
                            </a>
                        </span>
                    
                </div>
            </div>
        

        
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>Built by <a href="https://www.linkedin.com/in/oliviereeckhoutte/">Olivier Eeckhoutte</a>,
                Freelance @ LeanDeep <a href="https://leandeep.com/about/">(🍃 company)</a></span>
            <span>Siret: 83825337500011</span>
            <span><a href="https://leandeep.com/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss">
                        <path d="M4 11a9 9 0 0 1 9 9"></path>
                        <path d="M4 4a16 16 0 0 1 16 16"></path>
                        <circle cx="5" cy="19" r="1"></circle>
                    </svg></a></span>
        </div>
    </div>
</footer>
            
        </div>

        




<script type="text/javascript" src="https://leandeep.com/bundle.min.c184f8481b5847ad1a7d8aa775944fa063f118cb4df68f4eaa3826a2a2e16b26a1ad798f5160210f265c6fbb9a5f19b953fed066ae1ed1092d1858bcff13ae92.js" integrity="sha512-wYT4SBtYR60afYqndZRPoGPxGMtN9o9OqjgmoqLhayahrXmPUWAhDyZcb7uaXxm5U/7QZq4e0QktGFi8/xOukg=="></script>







    </body>
</html>
