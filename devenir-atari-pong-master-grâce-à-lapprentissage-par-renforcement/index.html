<!DOCTYPE html>
<html lang="fr">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author"
  content="Olivier Eeckhoutte">
<meta name="description"
  content="Qu&amp;rsquo;est-ce que l&amp;rsquo;apprentissage par renforcement ? L&amp;rsquo;apprentissage par renforcement est utilis√© dans l&amp;rsquo;intelligence artificielle pour enseigner aux ordinateurs comment prendre de meilleures d√©cisions en fonction des r√©compenses qu&amp;rsquo;ils re√ßoivent.
En d&amp;rsquo;autres termes, l&amp;rsquo;apprentissage par renforcement, c&amp;rsquo;est un peu comme apprendre √† faire quelque chose en obtenant des r√©compenses pour ses actions.
En trading par exemple, on √©value quelle strat√©gie va maximiser les r√©compenses qui sont le retour sur investissement. Les r√©compenses peuvent √™tre obtenues longtemps apr√®s une action.
Autre exemple, avec un jeu d&amp;rsquo;√©chec, on peut obtenir des r√©compenses mieux que ce qu&amp;rsquo;on aurait pu jouer simplement en sacrifiant des pi√®ces pour jouer un meilleur coup.
En apprentissage par renforcement, on cr√©e une politique qui d√©finit l&amp;rsquo;action qui maximisera les r√©compenses lorsqu&amp;rsquo;une action sera ex√©cut√©e en fonction de l&amp;rsquo;√©tat du syst√®me.
Atari Pong Pour coder mon r√©seau de neurones et faire de l&amp;rsquo;apprentissage par renforcement sur un cas pratique et simple, j&amp;rsquo;ai utilis√© le framework OpenAI gym.
&amp;ldquo;Gym is a toolkit for developing and comparing reinforcement learning algorithms&amp;rdquo;
Ce framework permet d&amp;rsquo;int√©ragir avec des jeux basiques Atari. J&amp;rsquo;ai choisi le jeu Atari Pong pour impl√©menter mon algorithme gradients de politique.
Voici 2 vid√©os que j&amp;rsquo;ai enregistr√© qui montrent des parties jou√©es entre un agent qui est l&amp;rsquo;ordinateur et un agent qui est pilot√© par un r√©seau de neurones.
Dans la premi√®re vid√©o, le r√©seau de neurones n&amp;rsquo;a pas encore √©t√© entra√Æn√©. Dans la seconde vid√©o, le r√©seau de neurones a √©t√© entra√Æn√© pendant des jours avec un algorithme policy gradients. " />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<script>
  
  
  if (!(window.location.host.startsWith("127.0.0.1")) && !(window.location.host.startsWith("localhost"))) {
    if (window.location.protocol != "https:") {
      console.log("Redirecting to https...")
      window.location.protocol = "https";
    }
  }
</script>


<link rel="canonical" href="https://leandeep.com/devenir-atari-pong-master-gr%C3%A2ce-%C3%A0-lapprentissage-par-renforcement/" />




<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">



<script src="https://fb.me/react-15.1.0.min.js"></script>
<script src="https://fb.me/react-dom-15.1.0.min.js"></script>
<style>
  .nav-search {
    display: none;
    -webkit-flex-grow: 1;
    -ms-flex-positive: 1;
    position: relative;
    width: 90%;
    height: 47px;
    margin-top: 20px;
    background-color: white;
    z-index: 1000;
  }

  .nav-search.active {
    box-shadow: 0 4px 4px rgba(79, 79, 79, 0.21);
  }

  .nav-search.active .search-dropdown {
    display: block;
  }

  .nav-search.active .search-input {
    -webkit-animation: expand-search-box-animation 0.5s forwards;
    animation: expand-search-box-animation 0.5s forwards;
  }

  .nav-search.active .search-input input {
    border-width: 2px;
  }

  .nav-search.active .search-input .close-search {
    display: inline-block;
  }

  .nav-search.active .search-input .search-dropdown {
    display: block;
  }

  .nav-search .search-input {
    transition: left 0.2s ease-in-out;
    transition: width 0s ease-in-out;
  }

  .nav-search .search-input .search-icon {
    position: absolute;
    left: 15px;
    top: 13px;
    z-index: 999;
    color: black;
  }

  .nav-search .search-input input {
    font: 16px/1.875 "Avenir Next W01", "Avenir Next", "Helvetica Neue", Helvetica, sans-serif;
    height: 50px;
    border: 1px solid #1b98f4;
    border-radius: 4px;
    min-width: 200px;
    width: 100%;
    padding-left: 50px;
    background-color: white;
  }

  .nav-search .search-input input:focus {
    outline: none;
  }

  .nav-search .search-input i.close-search {
    color: #1b98f4;
    display: none;
    position: absolute;
    right: 15px;
    top: 13px;
    cursor: pointer;
  }

  .search-dropdown {
    box-sizing: border-box;
    color: #B3B3B3;
    font: 14px/1.875 "Avenir Next W01", "Avenir Next", "Helvetica Neue", Helvetica, sans-serif;
    opacity: 1.00;
    padding: 20px;
    width: 100%;
    -webkit-animation: expand-search-dropdown-animation 0.5s forwards;
    animation: expand-search-dropdown-animation 0.5s forwards;
    overflow-y: scroll;
    max-height: 400px;
    border-radius: 0 0 4px 4px;
    background-color: #FCFCFC;
    border: 1px solid #E0E0E0;
    box-shadow: 1px 3px 4px rgba(0, 0, 0, 0.09);
    display: none;
    background-color: white;
  }

  .search-dropdown .small {
    -webkit-flex-basis: 35%;
    -ms-flex-preferred-size: 35%;
    flex-basis: 35%;
  }

  .search-dropdown .search-section .hits-blank {
    color: #666;
    text-align: center;
    padding-top: 20px;
  }

  .search-dropdown a {
    text-decoration: none;
    color: inherit;
    z-index: 2000;
  }

  .hit {
    border-bottom: 1px solid #E6E6E6;
    margin-bottom: 20px;
  }

  .hit .hit-title {
    color: #1b98f4;
    font-family: 'bt_mono', monospace;
    font-weight: 500;
    margin-bottom: 0;
    margin-top: 0;
    display: inline-block;
    font-size: 14px;
  }

  .hit .hit-description {
    text-decoration: none;
    color: black;
    font-size: 14px;
    display: block;
    margin-top: 3px;
  }

  .hit .hit-anchor {
    font-size: 13px;
    color: #666;
  }

  .hit .algolia-docsearch-suggestion--highlight {
    background-color: #FFE9A4;
  }

  .hit:last-child {
     
  }

  .ais-hits--item:last-child .hit {
    border: 0;
  }
</style>

<style>
  #app {
    display: none;
     
    border-radius: 10px;
    box-shadow: 2px 5px 12px -1px rgba(0, 0, 0, 0.56);
    padding: 20px;
    background-color: white;
    max-width: 500px;
    margin: 15px auto;
    text-align: center;
    min-height: 500px;
  }

  #app input {
    margin: 0 auto;
    float: none;
    width: 100%;
    max-width: 300px;
    padding: 5px 10px;
    border: 2px solid black;
  }

  #app ul {
    margin: 0;
    padding: 0;
  }

  #app li {
    text-align: left;
    padding: 5px 10px;
    width: 100%;
    max-width: 280px;
    margin: 1px auto;
    background-color: white;
    border: 1px solid black;
    list-style: none;
  }
</style>




<title>
  
  Devenir Atari Pong master gr√¢ce √† l&#39;apprentissage par renforcement :: Lean Deep Tech blog 
  
</title>



<link href="//cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
  type="text/css">



<link rel="stylesheet" href="https://leandeep.com/main.min.5a145ada8612b7e3e366691ddad4ba3ce6007aa2562c494a29f0b76b636f9789.css">




    <link rel="apple-touch-icon" sizes="180x180" href="https://leandeep.com/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://leandeep.com/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://leandeep.com/favicon-16x16.png">
    <link rel="manifest" href="https://leandeep.com/site.webmanifest">
    <link rel="mask-icon" href="https://leandeep.com/safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="https://leandeep.com/favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">

<meta itemprop="name" content="Devenir Atari Pong master gr√¢ce √† l&#39;apprentissage par renforcement">
<meta itemprop="description" content="Qu&rsquo;est-ce que l&rsquo;apprentissage par renforcement ?
L&rsquo;apprentissage par renforcement est utilis√© dans l&rsquo;intelligence artificielle pour enseigner aux ordinateurs comment prendre de meilleures d√©cisions en fonction des r√©compenses qu&rsquo;ils re√ßoivent.
En d&rsquo;autres termes, l&rsquo;apprentissage par renforcement, c&rsquo;est un peu comme apprendre √† faire quelque chose en obtenant des r√©compenses pour ses actions.
En trading par exemple, on √©value quelle strat√©gie va maximiser les r√©compenses qui sont le retour sur investissement. Les r√©compenses peuvent √™tre obtenues longtemps apr√®s une action.
Autre exemple, avec un jeu d&rsquo;√©chec, on peut obtenir des r√©compenses mieux que ce qu&rsquo;on aurait pu jouer simplement en sacrifiant des pi√®ces pour jouer un meilleur coup.

En apprentissage par renforcement, on cr√©e une politique qui d√©finit l&rsquo;action qui maximisera les r√©compenses lorsqu&rsquo;une action sera ex√©cut√©e en fonction de l&rsquo;√©tat du syst√®me.


Atari Pong
Pour coder mon r√©seau de neurones et faire de l&rsquo;apprentissage par renforcement sur un cas pratique et simple, j&rsquo;ai utilis√© le framework OpenAI gym.

&ldquo;Gym is a toolkit for developing and comparing reinforcement learning algorithms&rdquo;

Ce framework permet d&rsquo;int√©ragir avec des jeux basiques Atari.
J&rsquo;ai choisi le jeu Atari Pong pour impl√©menter mon algorithme gradients de politique.
Voici 2 vid√©os que j&rsquo;ai enregistr√© qui montrent des parties jou√©es entre un agent qui est l&rsquo;ordinateur et un agent qui est pilot√© par un r√©seau de neurones.

Dans la premi√®re vid√©o, le r√©seau de neurones n&rsquo;a pas encore √©t√© entra√Æn√©.


    
    







Dans la seconde vid√©o, le r√©seau de neurones a √©t√© entra√Æn√© pendant des jours avec un algorithme policy gradients.


    
    




"><meta itemprop="datePublished" content="2018-03-29T21:09:00+00:00" />
<meta itemprop="dateModified" content="2018-03-29T21:09:00+00:00" />
<meta itemprop="wordCount" content="846"><meta itemprop="image" content="https://leandeep.com"/>
<meta itemprop="keywords" content="Featured,Apprentissage Renforcement,Machine Learning," /><meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://leandeep.com"/>

<meta name="twitter:title" content="Devenir Atari Pong master gr√¢ce √† l&#39;apprentissage par renforcement"/>
<meta name="twitter:description" content="Qu&rsquo;est-ce que l&rsquo;apprentissage par renforcement ?
L&rsquo;apprentissage par renforcement est utilis√© dans l&rsquo;intelligence artificielle pour enseigner aux ordinateurs comment prendre de meilleures d√©cisions en fonction des r√©compenses qu&rsquo;ils re√ßoivent.
En d&rsquo;autres termes, l&rsquo;apprentissage par renforcement, c&rsquo;est un peu comme apprendre √† faire quelque chose en obtenant des r√©compenses pour ses actions.
En trading par exemple, on √©value quelle strat√©gie va maximiser les r√©compenses qui sont le retour sur investissement. Les r√©compenses peuvent √™tre obtenues longtemps apr√®s une action.
Autre exemple, avec un jeu d&rsquo;√©chec, on peut obtenir des r√©compenses mieux que ce qu&rsquo;on aurait pu jouer simplement en sacrifiant des pi√®ces pour jouer un meilleur coup.

En apprentissage par renforcement, on cr√©e une politique qui d√©finit l&rsquo;action qui maximisera les r√©compenses lorsqu&rsquo;une action sera ex√©cut√©e en fonction de l&rsquo;√©tat du syst√®me.


Atari Pong
Pour coder mon r√©seau de neurones et faire de l&rsquo;apprentissage par renforcement sur un cas pratique et simple, j&rsquo;ai utilis√© le framework OpenAI gym.

&ldquo;Gym is a toolkit for developing and comparing reinforcement learning algorithms&rdquo;

Ce framework permet d&rsquo;int√©ragir avec des jeux basiques Atari.
J&rsquo;ai choisi le jeu Atari Pong pour impl√©menter mon algorithme gradients de politique.
Voici 2 vid√©os que j&rsquo;ai enregistr√© qui montrent des parties jou√©es entre un agent qui est l&rsquo;ordinateur et un agent qui est pilot√© par un r√©seau de neurones.

Dans la premi√®re vid√©o, le r√©seau de neurones n&rsquo;a pas encore √©t√© entra√Æn√©.


    
    







Dans la seconde vid√©o, le r√©seau de neurones a √©t√© entra√Æn√© pendant des jours avec un algorithme policy gradients.


    
    




"/>





<meta property="article:published_time" content="2018-03-29 21:09:00 &#43;0000 UTC" />







    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://leandeep.com/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">$ cd /home/leandeep</span>
            <span class="logo__cursor" style=""></span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://leandeep.com/events/">Featured Articles</a></li><li><a href="https://leandeep.com/posts/">All Articles</a></li><li><a href="https://leandeep.com/about/">About</a></li><li><a href="https://leandeep.com/finance/">Finance</a></li><li><a href="https://leandeep.com/notebooks/">ML Notebooks</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>

            <div class="content">
                
    <main class="post">

        <div class="post-info">
            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>4 minutes

            

            </p>
        </div>

        <article>
            <h1 class="post-title">
                <a href="https://leandeep.com/devenir-atari-pong-master-gr%C3%A2ce-%C3%A0-lapprentissage-par-renforcement/">Devenir Atari Pong master gr√¢ce √† l&rsquo;apprentissage par renforcement</a>
            </h1>

            

            <div class="post-content">
                <h2 id="quest-ce-que-lapprentissage-par-renforcement-">Qu&rsquo;est-ce que l&rsquo;apprentissage par renforcement ?</h2>
<p>L&rsquo;apprentissage par renforcement est utilis√© dans l&rsquo;intelligence artificielle pour enseigner aux ordinateurs comment prendre de meilleures d√©cisions en fonction des r√©compenses qu&rsquo;ils re√ßoivent.</p>
<p>En d&rsquo;autres termes, l&rsquo;apprentissage par renforcement, c&rsquo;est un peu comme apprendre √† faire quelque chose en obtenant des r√©compenses pour ses actions.</p>
<p>En trading par exemple, on √©value quelle strat√©gie va maximiser les r√©compenses qui sont le retour sur investissement. Les r√©compenses peuvent √™tre obtenues longtemps apr√®s une action.</p>
<p>Autre exemple, avec un jeu d&rsquo;√©chec, on peut obtenir des r√©compenses mieux que ce qu&rsquo;on aurait pu jouer simplement en sacrifiant des pi√®ces pour jouer un meilleur coup.</p>
<blockquote>
<p>En apprentissage par renforcement, on cr√©e une <strong>politique</strong> qui d√©finit l&rsquo;<strong>action</strong> qui maximisera les <strong>r√©compenses</strong> lorsqu&rsquo;une action sera ex√©cut√©e en fonction de l&rsquo;√©tat du syst√®me.</p>
</blockquote>
<br/>
<h2 id="atari-pong">Atari Pong</h2>
<p>Pour coder mon r√©seau de neurones et faire de l&rsquo;apprentissage par renforcement sur un cas pratique et simple, j&rsquo;ai utilis√© le framework <a href="https://gym.openai.com/docs/">OpenAI gym</a>.</p>
<blockquote>
<p>&ldquo;Gym is a toolkit for developing and comparing reinforcement learning algorithms&rdquo;</p>
</blockquote>
<p>Ce framework permet d&rsquo;int√©ragir avec des jeux basiques Atari.
J&rsquo;ai choisi le jeu Atari Pong pour impl√©menter mon algorithme gradients de politique.</p>
<p>Voici 2 vid√©os que j&rsquo;ai enregistr√© qui montrent des parties jou√©es entre un agent qui est l&rsquo;ordinateur et un agent qui est pilot√© par un r√©seau de neurones.</p>
<ul>
<li>Dans la premi√®re vid√©o, le r√©seau de neurones n&rsquo;a pas encore √©t√© entra√Æn√©.</li>
</ul>

    <iframe 
        width="100%" 
        height="400" 
        src="//www.youtube.com/embed/fmxJNCf4REY?rel=0" 
        frameborder="0" 
        allow="autoplay; encrypted-media" 
        allowfullscreen>
    </iframe>




<p><br/>
<br/></p>
<ul>
<li>Dans la seconde vid√©o, le r√©seau de neurones a √©t√© entra√Æn√© pendant des jours avec un algorithme policy gradients.</li>
</ul>

    <iframe 
        width="100%" 
        height="400" 
        src="//www.youtube.com/embed/yo2c-SCFL-M?rel=0" 
        frameborder="0" 
        allow="autoplay; encrypted-media" 
        allowfullscreen>
    </iframe>




<!-- raw HTML omitted -->
<br/>
<br/>
<h2 id="algorithmes-gradients-de-politiquenbsppolicy-gradients">Algorithmes gradients de politique¬†<em>(Policy Gradients)</em></h2>
<p>Les algorithmes de gradient de politique sont un type d&rsquo;algorithme d&rsquo;apprentissage par renforcement qui utilisent la descente de gradient pour apprendre une politique qui maximise une r√©compense donn√©e.</p>
<p>Les algorithmes gradients de politique sont utilis√©s pour apprendre √† un ordinateur comment prendre de meilleures d√©cisions pour accomplir une t√¢che sp√©cifique. Voici comment √ßa marche :</p>
<ul>
<li>
<p>Objectif: Tout d&rsquo;abord, l&rsquo;ordinateur a une t√¢che √† accomplir, comme jouer √† un jeu ou piloter un robot.</p>
</li>
<li>
<p>Actions: Il peut effectuer diff√©rentes actions pour essayer de bien faire cette t√¢che. Par exemple, dans un jeu vid√©o, il peut d√©cider de bouger vers la gauche ou vers la droite.</p>
</li>
<li>
<p>Politique: L&rsquo;ordinateur a une sorte de plan, appel√© &ldquo;politique&rdquo;, qui lui dit quelles actions prendre dans diff√©rentes situations. Au d√©but, ce plan peut √™tre al√©atoire, c&rsquo;est-√†-dire qu&rsquo;il fait des choses au hasard.</p>
</li>
<li>
<p>R√©compense: √Ä chaque fois qu&rsquo;il fait quelque chose, il re√ßoit une r√©compense. Si ce qu&rsquo;il fait est bon, la r√©compense est √©lev√©e ; s&rsquo;il se trompe, la r√©compense est faible.</p>
</li>
<li>
<p>Apprentissage: Maintenant, l&rsquo;ordinateur veut devenir meilleur. Il regarde les r√©compenses qu&rsquo;il a re√ßues et ajuste sa politique. Si une action a conduit √† une grande r√©compense, il est plus susceptible de faire cette action dans le futur. Si une action a conduit √† une petite r√©compense, il est moins susceptible de la faire √† l&rsquo;avenir.</p>
</li>
<li>
<p>R√©p√©tition: Il continue √† jouer ou √† effectuer des actions, √† recevoir des r√©compenses, √† ajuster sa politique, encore et encore, jusqu&rsquo;√† ce qu&rsquo;il devienne vraiment bon √† la t√¢che.</p>
</li>
</ul>
<p>Donc, les algorithmes gradients de politique sont comme un apprentissage par essais et erreurs, o√π l&rsquo;ordinateur apprend √† faire de meilleures actions en se basant sur les r√©compenses qu&rsquo;il re√ßoit. C&rsquo;est un peu comme quand tu essaies de faire quelque chose, comme dessiner, et que tu t&rsquo;am√©liores en pratiquant et en apprenant de tes erreurs.</p>
<br/>
<p>Voici un tr√®s bon cours pour comprendre en d√©tail comment fonctionnent ces algorithmes:
<a href="http://rll.berkeley.edu/deeprlcourse/">http://rll.berkeley.edu/deeprlcourse/</a> (voir lecture 4 policy gradient).</p>
<p>Il y a √©galement cette vid√©o qui traite sp√©cifiquement de ces algorithmes:</p>

    <iframe 
        width="100%" 
        height="400" 
        src="//www.youtube.com/embed/oPGVsoBonLM?rel=0" 
        frameborder="0" 
        allow="autoplay; encrypted-media" 
        allowfullscreen>
    </iframe>




<p>En synth√®se, voici un r√©sum√© de l&rsquo;algo impl√©ment√©:</p>
<ul>
<li>
<p>On commence par laisser la politique du r√©seau de neurones jouer plusieurs parties de Pong et √† chaque √©tape on calcule les gradients qui augmenteraient la probabilit√© de l&rsquo;action choisie. Au d√©but, on ne fait rien d&rsquo;autre que calculer les gradients.</p>
</li>
<li>
<p>Apr√®s plusieurs parties, on calcule le score de chaque action. On √©value une action en fonction de la somme de toutes les r√©compenses qui s&rsquo;ensuivent en appliquant √† chaque √©tape un taux de rabais (<strong>discount rate</strong>) r.
Par exemple, si on consid√®re que notre agent va 3 fois de suite vers le haut et qu&rsquo;on prend les hypoth√®ses suivantes:</p>
<ul>
<li>Il re√ßoit +10 comme r√©compense apr√®s la premi√®re √©tape</li>
<li>0 apr√®s la deuxi√®me</li>
<li>-50 apr√®s la troisi√®me</li>
<li>r = 0,8 (plus le taux de rabais est proche de 1 et plus les r√©compenses arrivant tardivement comptent autant que les r√©compenses imm√©diates. G√©n√©ralemement r varie entre 0,95 et 0,99.</li>
<li>Conclusion: le score total est de 10 * r^0 + 0 * r^1 + (-50) * r^2 = -22</li>
</ul>
</li>
<li>
<p>Si le score est positif, cela signifie que l&rsquo;action √©tait bonne. On applique les gradients calcul√©s lors de la premi√®re √©tape. Appliquer les gradients signifie que l&rsquo;on multiplie son vecteur par le score de l&rsquo;action correspondante. L&rsquo;action sera ainsi davantage utilis√©e dans le futur.
Si l&rsquo;action n&rsquo;√©tait pas bonne, on applique les gradients oppos√©s.</p>
</li>
<li>
<p>Pour terminer, on calcule la moyenne de tous les vecteurs de gradients obtenus et on l&rsquo;utilise pour effectuer une √©tape de descente de gradient.</p>
</li>
</ul>
            </div>
        </article>

        <hr />

        <div class="post-info">
                <p>
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="https://leandeep.com/tags/featured">Featured</a></span><span class="tag"><a href="https://leandeep.com/tags/apprentissage-renforcement">Apprentissage Renforcement</a></span><span class="tag"><a href="https://leandeep.com/tags/machine-learning">Machine Learning</a></span>
                </p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>846 Mots</p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>29 mars. 2018</p>
        </div>

        
            <div class="pagination">
                <div class="pagination__title">
                    <span class="pagination__title-h"></span>
                    <hr />
                </div>

                <div class="pagination__buttons">
                    
                        <span class="button previous">
                            <a href="https://leandeep.com/pytest-tricks/">
                                <span class="button__icon">‚Üê</span>
                                <span class="button__text">Pytest tricks</span>
                            </a>
                        </span>
                    

                    
                        <span class="button next">
                            <a href="https://leandeep.com/voiture-autonome-avec-unity-et-keras/">
                                <span class="button__text">Voiture autonome avec Unity et Keras</span>
                                <span class="button__icon">‚Üí</span>
                            </a>
                        </span>
                    
                </div>
            </div>
        

        
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>Built by <a href="https://www.linkedin.com/in/oliviereeckhoutte/">Olivier Eeckhoutte</a>,
                Freelance @ LeanDeep <a href="https://leandeep.com/about/">(üçÉ company)</a></span>
            <span>Siret: 83825337500011</span>
            <span><a href="https://leandeep.com/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss">
                        <path d="M4 11a9 9 0 0 1 9 9"></path>
                        <path d="M4 4a16 16 0 0 1 16 16"></path>
                        <circle cx="5" cy="19" r="1"></circle>
                    </svg></a></span>
        </div>
    </div>
</footer>
            
        </div>

        




<script type="text/javascript" src="https://leandeep.com/bundle.min.c184f8481b5847ad1a7d8aa775944fa063f118cb4df68f4eaa3826a2a2e16b26a1ad798f5160210f265c6fbb9a5f19b953fed066ae1ed1092d1858bcff13ae92.js" integrity="sha512-wYT4SBtYR60afYqndZRPoGPxGMtN9o9OqjgmoqLhayahrXmPUWAhDyZcb7uaXxm5U/7QZq4e0QktGFi8/xOukg=="></script>







    </body>
</html>
