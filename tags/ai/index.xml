<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on Lean Deep Tech blog</title>
    <link>https://leandeep.com/tags/ai/</link>
    <description>Recent content in AI on Lean Deep Tech blog</description>
    <generator>Hugo</generator>
    <language>fr</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Sun, 25 Jan 2026 12:08:20 +0200</lastBuildDate>
    <atom:link href="https://leandeep.com/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Installer et tester Ollama sur Ubuntu avec GPU support</title>
      <link>https://leandeep.com/installer-et-tester-ollama-sur-ubuntu-avec-gpu-support/</link>
      <pubDate>Sun, 25 Jan 2026 12:08:20 +0200</pubDate>
      <guid>https://leandeep.com/installer-et-tester-ollama-sur-ubuntu-avec-gpu-support/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Verifier que le GPU est accessible&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;nvidia-smi&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;br/&gt;&#xA;&lt;p&gt;&lt;strong&gt;Installer Ollama&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;curl -fsSL https://ollama.com/install.sh | sh&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;blockquote&gt;&#xA;&lt;p&gt;Cela va installer:&lt;br/&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;/usr/bin/ollama&lt;br/&gt;&lt;/li&gt;&#xA;&lt;li&gt;le service systemd&lt;br/&gt;&lt;/li&gt;&#xA;&lt;li&gt;le support CUDA si driver NVIDIA détecté&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/blockquote&gt;&#xA;&lt;br/&gt;&#xA;&lt;p&gt;&lt;strong&gt;Vérifier l&amp;rsquo;installation&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;ollama --version&#xA;ollama --info # Pour vérifier GPU visible&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;br/&gt;&#xA;&lt;p&gt;&lt;strong&gt;Forcer Ollama à utiliser uniquement un eGPU&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Identifier l&amp;rsquo;ID du eGPU**&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;nvidia-smi -L&#xA;GPU 0: RTX 5090 (internal)&#xA;GPU 1: RTX 5090 (internal 2)&#xA;GPU 2: RTX 4080 (eGPU)&#xA;# choisir 2&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;br/&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=2 ollama run llama3&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;br/&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo systemctl edit ollama&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Ajoute:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using AI and IoT to Safely Automate Fireplace Fire Starting</title>
      <link>https://leandeep.com/using-ai-and-iot-to-safely-automate-fireplace-fire-starting/</link>
      <pubDate>Mon, 09 Jun 2025 22:32:00 +0200</pubDate>
      <guid>https://leandeep.com/using-ai-and-iot-to-safely-automate-fireplace-fire-starting/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m currently building an IoT device that will automatically start fires in my traditional chimney at scheduled times during cold winter days.&#xA;The idea is to start a fire one hour before I wake up, while I&amp;rsquo;m still asleep, to warm up the house.&#xA;Unlike a smart stove, a traditional fireplace can&amp;rsquo;t be controlled remotely. It’s old-school with a twist of AI, code, and electronics.&#xA;Everything is getting automated! Here are the first two videos showing the progress of my side project.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Utiliser Ollama sur Ubuntu 22 via Docker et faire du LLM scraping</title>
      <link>https://leandeep.com/utiliser-ollama-sur-ubuntu-22-via-docker-et-faire-du-llm-scraping/</link>
      <pubDate>Sun, 04 May 2025 23:32:00 +0200</pubDate>
      <guid>https://leandeep.com/utiliser-ollama-sur-ubuntu-22-via-docker-et-faire-du-llm-scraping/</guid>
      <description>&lt;p&gt;Dans cet article, nous allons voir comment utiliser Ollama sur Ubuntu 22 via Docker tout en tirant parti du GPU connecté au serveur dans le but de faire du scraping en posant des questions à son LLM auto-hébergé.&lt;/p&gt;&#xA;&lt;br/&gt;&#xA;&lt;p&gt;&lt;strong&gt;Pré-requis&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;nvidia-smi&lt;/code&gt; déjà installé&lt;/li&gt;&#xA;&lt;li&gt;Cuda installé&lt;/li&gt;&#xA;&lt;li&gt;Docker installé&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;br/&gt;&#xA;&lt;p&gt;&lt;strong&gt;Installation du NVIDIA Container Toolkit&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo apt install -y nvidia-container-toolkit&#xA;sudo nvidia-ctk runtime configure --runtime=docker&#xA;sudo systemctl restart docker&#xA;# Vérification&#xA;docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu20.04 nvidia-smi&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;br/&gt;&#xA;&lt;p&gt;&lt;strong&gt;Docker compose Ollama avec accès GPU&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Construire une voiture téléguidée autonome - part 1</title>
      <link>https://leandeep.com/construire-une-voiture-t%C3%A9l%C3%A9guid%C3%A9e-autonome-part-1/</link>
      <pubDate>Fri, 21 Dec 2018 22:30:00 +0000</pubDate>
      <guid>https://leandeep.com/construire-une-voiture-t%C3%A9l%C3%A9guid%C3%A9e-autonome-part-1/</guid>
      <description>&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;&#xA;&lt;p&gt;Mon objectif est de construire une voiture téléguidée autonome. Réaliser ce projet me permettra de développer davantage mes compétences sur le sujet de la conduite autonome et plus généralement du Deep Learning. J&amp;rsquo;ai déjà réalisé un premier projet sur ce sujet (j&amp;rsquo;ai publié un article et une vidéo Youtube) mais mon expérimentation utilisait le simulateur Unity. Maintenant avec ce nouveau projet, je veux sortir du virtuel et aller un cran plus loin en passant au monde réel.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
