<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on Lean Deep Tech blog</title>
    <link>https://leandeep.com/tags/ai/</link>
    <description>Recent content in AI on Lean Deep Tech blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>fr</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Mon, 09 Jun 2025 22:32:00 +0200</lastBuildDate><atom:link href="https://leandeep.com/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Leveraging AI to play with fire safely</title>
      <link>https://leandeep.com/leveraging-ai-to-play-with-fire-safely/</link>
      <pubDate>Mon, 09 Jun 2025 22:32:00 +0200</pubDate>
      
      <guid>https://leandeep.com/leveraging-ai-to-play-with-fire-safely/</guid>
      <description>I&amp;rsquo;m currently building an IoT device that will automatically start fires in my traditional chimney at scheduled times during cold winter days. The idea is to start a fire one hour before I wake up, while I&amp;rsquo;m still asleep, to warm up the house. Unlike a smart stove, a traditional fireplace can&amp;rsquo;t be controlled remotely. It’s old-school with a twist of AI, code, and electronics. Everything is getting automated! Here are the first two videos showing the progress of my side project.</description>
    </item>
    
    <item>
      <title>Utiliser Ollama sur Ubuntu 22 via Docker et faire du LLM scraping</title>
      <link>https://leandeep.com/utiliser-ollama-sur-ubuntu-22-via-docker-et-faire-du-llm-scraping/</link>
      <pubDate>Sun, 04 May 2025 23:32:00 +0200</pubDate>
      
      <guid>https://leandeep.com/utiliser-ollama-sur-ubuntu-22-via-docker-et-faire-du-llm-scraping/</guid>
      <description>Dans cet article, nous allons voir comment utiliser Ollama sur Ubuntu 22 via Docker tout en tirant parti du GPU connecté au serveur dans le but de faire du scraping en posant des questions à son LLM auto-hébergé.
Pré-requis
nvidia-smi déjà installé Cuda installé Docker installé Installation du NVIDIA Container Toolkit
sudo apt install -y nvidia-container-toolkit sudo nvidia-ctk runtime configure --runtime=docker sudo systemctl restart docker # Vérification docker run --rm --gpus all nvidia/cuda:12.</description>
    </item>
    
    <item>
      <title>Construire une voiture téléguidée autonome - part 1</title>
      <link>https://leandeep.com/construire-une-voiture-t%C3%A9l%C3%A9guid%C3%A9e-autonome-part-1/</link>
      <pubDate>Fri, 21 Dec 2018 22:30:00 +0000</pubDate>
      
      <guid>https://leandeep.com/construire-une-voiture-t%C3%A9l%C3%A9guid%C3%A9e-autonome-part-1/</guid>
      <description>Introduction Mon objectif est de construire une voiture téléguidée autonome. Réaliser ce projet me permettra de développer davantage mes compétences sur le sujet de la conduite autonome et plus généralement du Deep Learning. J&amp;rsquo;ai déjà réalisé un premier projet sur ce sujet (j&amp;rsquo;ai publié un article et une vidéo Youtube) mais mon expérimentation utilisait le simulateur Unity. Maintenant avec ce nouveau projet, je veux sortir du virtuel et aller un cran plus loin en passant au monde réel.</description>
    </item>
    
  </channel>
</rss>
