<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on Lean Deep Tech blog</title>
    <link>https://leandeep.com/tags/llm/</link>
    <description>Recent content in LLM on Lean Deep Tech blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>fr</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Sun, 04 May 2025 23:32:00 +0200</lastBuildDate><atom:link href="https://leandeep.com/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Utiliser Ollama sur Ubuntu 22 via Docker et faire du LLM scraping</title>
      <link>https://leandeep.com/utiliser-ollama-sur-ubuntu-22-via-docker-et-faire-du-llm-scraping/</link>
      <pubDate>Sun, 04 May 2025 23:32:00 +0200</pubDate>
      
      <guid>https://leandeep.com/utiliser-ollama-sur-ubuntu-22-via-docker-et-faire-du-llm-scraping/</guid>
      <description>Dans cet article, nous allons voir comment utiliser Ollama sur Ubuntu 22 via Docker tout en tirant parti du GPU connecté au serveur dans le but de faire du scraping en posant des questions à son LLM auto-hébergé.
Pré-requis
nvidia-smi déjà installé Cuda installé Docker installé Installation du NVIDIA Container Toolkit
sudo apt install -y nvidia-container-toolkit sudo nvidia-ctk runtime configure --runtime=docker sudo systemctl restart docker # Vérification docker run --rm --gpus all nvidia/cuda:12.</description>
    </item>
    
  </channel>
</rss>
