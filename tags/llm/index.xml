<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM on Lean Deep Tech blog</title>
    <link>https://leandeep.com/tags/llm/</link>
    <description>Recent content in LLM on Lean Deep Tech blog</description>
    <generator>Hugo</generator>
    <language>fr</language>
    <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
    <lastBuildDate>Sun, 04 May 2025 23:32:00 +0200</lastBuildDate>
    <atom:link href="https://leandeep.com/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Utiliser Ollama sur Ubuntu 22 via Docker et faire du LLM scraping</title>
      <link>https://leandeep.com/utiliser-ollama-sur-ubuntu-22-via-docker-et-faire-du-llm-scraping/</link>
      <pubDate>Sun, 04 May 2025 23:32:00 +0200</pubDate>
      <guid>https://leandeep.com/utiliser-ollama-sur-ubuntu-22-via-docker-et-faire-du-llm-scraping/</guid>
      <description>&lt;p&gt;Dans cet article, nous allons voir comment utiliser Ollama sur Ubuntu 22 via Docker tout en tirant parti du GPU connecté au serveur dans le but de faire du scraping en posant des questions à son LLM auto-hébergé.&lt;/p&gt;&#xA;&lt;br/&gt;&#xA;&lt;p&gt;&lt;strong&gt;Pré-requis&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;nvidia-smi&lt;/code&gt; déjà installé&lt;/li&gt;&#xA;&lt;li&gt;Cuda installé&lt;/li&gt;&#xA;&lt;li&gt;Docker installé&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;br/&gt;&#xA;&lt;p&gt;&lt;strong&gt;Installation du NVIDIA Container Toolkit&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;sudo apt install -y nvidia-container-toolkit&#xA;sudo nvidia-ctk runtime configure --runtime=docker&#xA;sudo systemctl restart docker&#xA;# Vérification&#xA;docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu20.04 nvidia-smi&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;br/&gt;&#xA;&lt;p&gt;&lt;strong&gt;Docker compose Ollama avec accès GPU&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
