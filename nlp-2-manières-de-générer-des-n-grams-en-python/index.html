<!DOCTYPE html>
<html lang="fr">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author"
  content="Olivier Eeckhoutte">
<meta name="description"
  content="Introduction Dans une phrase, les N-grams sont des s√©quences de N-mots adjacents. N peut √™tre 1 ou 2 ou toute autre entier positif. En g√©n√©ral N n&amp;rsquo;est pas tr√®s grand car ces N-grams apparaissent rarement plusieurs fois.
On utilise ces N-grams en Machine Learning dans les sujets qui traitent du Natural Language Processing. Plus pr√©cis√©ment, on les retrouve dans les sujets de classification de textes. On peut utiliser des bi-grams ou tri-grams comme features pour repr√©senter nos documents en plus d&amp;rsquo;utiliser des tokens individuels trouv√©s dans le corpus." />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="#252627" />
<script>
  
  
  if (!(window.location.host.startsWith("127.0.0.1")) && !(window.location.host.startsWith("localhost"))) {
    if (window.location.protocol != "https:") {
      console.log("Redirecting to https...")
      window.location.protocol = "https";
    }
  }
</script>


<link rel="canonical" href="https://leandeep.com/nlp-2-mani%C3%A8res-de-g%C3%A9n%C3%A9rer-des-n-grams-en-python/" />



<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

<style>
  #app {
    display: none;
    border-radius: 10px;
    box-shadow: 2px 5px 12px -1px rgba(0, 0, 0, 0.56);
    padding: 20px;
    background-color: white;
    max-width: 500px;
    margin: 15px auto;
    text-align: center;
    min-height: 500px;
  }

  #app input {
    margin: 0 auto;
    float: none;
    width: 100%;
    max-width: 300px;
    padding: 5px 10px;
    border: 2px solid black;
  }

  #app ul {
    margin: 0;
    padding: 0;
  }

  #app li {
    text-align: left;
    padding: 5px 10px;
    width: 100%;
    max-width: 280px;
    margin: 1px auto;
    background-color: white;
    border: 1px solid black;
    list-style: none;
  }
</style>




<title>
  
  [NLP] 2 mani√®res de g√©n√©rer des N-grams en Python  :: Lean Deep Tech blog 
  
</title>



<link href="//cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
  type="text/css">



<link rel="stylesheet" href="https://leandeep.com/main.min.44eaa49e743eabd51724579f1d2ece0dac6f56215301d6961ca74092199d4a05.css">




    <link rel="apple-touch-icon" sizes="180x180" href="https://leandeep.com/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://leandeep.com/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://leandeep.com/favicon-16x16.png">
    <link rel="manifest" href="https://leandeep.com/site.webmanifest">
    <link rel="mask-icon" href="https://leandeep.com/safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="https://leandeep.com/favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">

<meta itemprop="name" content="[NLP] 2 mani√®res de g√©n√©rer des N-grams en Python ">
<meta itemprop="description" content="Introduction Dans une phrase, les N-grams sont des s√©quences de N-mots adjacents. N peut √™tre 1 ou 2 ou toute autre entier positif. En g√©n√©ral N n&rsquo;est pas tr√®s grand car ces N-grams apparaissent rarement plusieurs fois.
On utilise ces N-grams en Machine Learning dans les sujets qui traitent du Natural Language Processing. Plus pr√©cis√©ment, on les retrouve dans les sujets de classification de textes. On peut utiliser des bi-grams ou tri-grams comme features pour repr√©senter nos documents en plus d&rsquo;utiliser des tokens individuels trouv√©s dans le corpus."><meta itemprop="datePublished" content="2019-09-03T07:48:00+00:00" />
<meta itemprop="dateModified" content="2019-09-03T07:48:00+00:00" />
<meta itemprop="wordCount" content="665"><meta itemprop="image" content="https://leandeep.com"/>
<meta itemprop="keywords" content="NLP,Machine Learning,Ngrams," /><meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://leandeep.com"/>

<meta name="twitter:title" content="[NLP] 2 mani√®res de g√©n√©rer des N-grams en Python "/>
<meta name="twitter:description" content="Introduction Dans une phrase, les N-grams sont des s√©quences de N-mots adjacents. N peut √™tre 1 ou 2 ou toute autre entier positif. En g√©n√©ral N n&rsquo;est pas tr√®s grand car ces N-grams apparaissent rarement plusieurs fois.
On utilise ces N-grams en Machine Learning dans les sujets qui traitent du Natural Language Processing. Plus pr√©cis√©ment, on les retrouve dans les sujets de classification de textes. On peut utiliser des bi-grams ou tri-grams comme features pour repr√©senter nos documents en plus d&rsquo;utiliser des tokens individuels trouv√©s dans le corpus."/>





<meta property="article:published_time" content="2019-09-03 07:48:00 &#43;0000 UTC" />







    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="https://leandeep.com/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">$ cd /home/leandeep</span>
            <span class="logo__cursor" style=""></span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://leandeep.com/events/">Featured Articles</a></li><li><a href="https://leandeep.com/posts/">All Articles</a></li><li><a href="https://leandeep.com/about/">About</a></li><li><a href="https://leandeep.com/finance/">Finance</a></li><li><a href="https://leandeep.com/notebooks/">ML Notebooks</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>

            <div class="content">
                
    <main class="post">

        <div class="post-info">
            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>4 minutes

            

            </p>
        </div>

        <article>
            <h1 class="post-title">
                <a href="https://leandeep.com/nlp-2-mani%C3%A8res-de-g%C3%A9n%C3%A9rer-des-n-grams-en-python/">[NLP] 2 mani√®res de g√©n√©rer des N-grams en Python</a>
            </h1>

            

            <div class="post-content">
                <h2 id="introduction">Introduction</h2>
<p>Dans une phrase, les N-grams sont des s√©quences de N-mots adjacents. N peut √™tre 1 ou 2 ou toute autre entier positif. En g√©n√©ral N n&rsquo;est pas tr√®s grand car ces N-grams apparaissent rarement plusieurs fois.</p>
<p>On utilise ces N-grams en Machine Learning dans les sujets qui traitent du Natural Language Processing. Plus pr√©cis√©ment, on les retrouve dans les sujets de classification de textes. On peut utiliser des bi-grams ou tri-grams comme <em>features</em> pour repr√©senter nos documents en plus d&rsquo;utiliser des tokens individuels trouv√©s dans le corpus.</p>
<p>Dans cet article, nous allons voir comment g√©n√©rer en Python des N-grams √† partir de phrases en entr√©e.</p>
<br/>
<h2 id="n-grams-en-pur-python">N-grams en pur Python</h2>
<p>Partons de la phrase suivante et transformons la en N-grams:</p>
<pre tabindex="0"><code>s = &#34;On utilise ces N-grams en Machine Learning dans les sujets qui traitent du Natural Language &#34; \ 
    &#34;Processing et en particulier dans les sujets de classification de textes.&#34;
</code></pre><p>Si on transforme cette phrase en bi-grams on va obtenir l&rsquo;output suivant:</p>
<pre tabindex="0"><code>[
    &#34;On utilise&#34;,
    &#34;utilise ces&#34;,
    &#34;ces n&#34;,
    &#34;n grams&#34;,
    &#34;grams en&#34;,
	...
]
</code></pre><p>Pour obtenir le r√©sultat pr√©c√©dent, on peut utiliser le code Python suivant:</p>
<pre tabindex="0"><code>import re

def generate_ngrams(s, n):
    # Convert to lowercases
    s = s.lower()
    
    # Replace all non-alphanumeric characters with spaces
    s = re.sub(r&#39;[^a-zA-Z0-9\s]&#39;, &#39; &#39;, s)
    
    # Break sentence in the token, remove empty tokens
    tokens = [token for token in s.split(&#34; &#34;) if token != &#34;&#34;]
    
    # Use the zip function to help us generate n-grams
    # Concatentate the tokens into ngrams and return
    ngrams = zip(*[token[i:] for i in range(n)])
    return [&#34; &#34;.join(ngram) for ngram in ngrams]
</code></pre><p>Si on applique la fonction suivante sur notre phrase d&rsquo;entr√©e avec N=4, on obtient le r√©sultat suivant:</p>
<pre tabindex="0"><code>&gt;&gt;&gt; generate_ngrams(s, n=4)
[&#39;on utilise ces n&#39;, &#39;utilise ces n grams&#39;, &#39;ces n grams en&#39;, &#39;n grams en machine&#39;, &#39;grams en machine learning&#39;, &#39;en machine learning dans&#39;, &#39;machine learning dans les&#39;, &#39;learning dans les sujets&#39;, &#39;dans les sujets qui&#39;, &#39;les sujets qui traitent&#39;, &#39;sujets qui traitent du&#39;, &#39;qui traitent du natural&#39;, &#39;traitent du natural language&#39;, &#39;du natural language processing&#39;, &#39;natural language processing et&#39;, &#39;language processing et en&#39;, &#39;processing et en particulier&#39;, &#39;et en particulier dans&#39;, &#39;en particulier dans les&#39;, &#39;particulier dans les sujets&#39;, &#39;dans les sujets de&#39;, &#39;les sujets de classification&#39;, &#39;sujets de classification de&#39;, &#39;de classification de textes&#39;]
</code></pre><p>La fonction pr√©c√©dente utilise la fonction <code>zip</code> qui cr√©e un <em>generator</em> qui aggr√©ge les √©l√©ments de plusieurs listes.</p>
<p>Plus de d√©tails dans la section de code comment√©e:</p>
<pre tabindex="0"><code># phrase d&#39;entr√©e
s = &#34;un deux trois quatre cinq&#34;

tokens = s.split(&#34; &#34;)
# tokens = [&#34;un&#34;, &#34;deux&#34;, &#34;trois&#34;, &#34;quatre&#34;, &#34;cinq&#34;]

sequences = [tokens[i:] for i in range(3)]
# Cette ligne g√©n√®re des s√©quences depuis diff√©rents √©l√©ments de la liste tokens
range(x) d√©finit combien de s√©quences on veut g√©n√©rer
#
# sequences = [
#   [&#39;un&#39;, &#39;deux&#39;, &#39;trois&#39;, &#39;quatre&#39;, &#39;cinq&#39;],
#   [&#39;deux&#39;, &#39;trois&#39;, &#39;quatre&#39;, &#39;cinq&#39;],
#   [&#39;trois&#39;, &#39;quatre&#39;, &#39;cinq&#39;]]

bigrams = zip(*sequences)
# La fonction zip prend les 3 s√©quences comme une liste d&#39;input gr√¢ce √† l&#39;op√©rateur *. 
# Pour info, cette syntaxe revient au m√™me que zip(sequences[0], sequences[1], sequences[2]).
# Chaquee tuple que cette fonction zip retourne contient un √©l√©ment de chaque s√©quence.
# Comme il n&#39;y a que 3 √©l√©ments dans la derni√®res s√©quence, il n&#39;y a que 3 tuples retourn√©s par la fonction zip
</code></pre><br/>
<h2 id="n-grams-avec-nltk">N-grams avec NLTK</h2>
<p>Au lieu d&rsquo;utiliser la m√©thode pr√©c√©dente pour g√©n√©rer des N-grams, on peut se simplifier la vie en utilisant la librairie <code>NLTK (Natural Language Toolkit)</code> sp√©cialis√©e comme son nom l&rsquo;indique dans le NLP.</p>
<p>Avec le code suivant, on peut g√©n√©rer des N-grams, tout comme on l&rsquo;a fait avec la m√©thode <code>generate_ngrams()</code>.</p>
<pre tabindex="0"><code>import re
from nltk.util import ngrams

s = s.lower()
s = re.sub(r&#39;[^a-zA-Z0-9\s]&#39;, &#39; &#39;, s)
tokens = [token for token in s.split(&#34; &#34;) if token != &#34;&#34;]
output = list(ngrams(tokens, 5))
</code></pre><blockquote>
<p>Si vous rencontrez l&rsquo;erreur suivante avec NLTK <code>CERTIFICATE_VERIFY_FAILED] certificate verify failed:</code>, voici les commandes Python √† ex√©cuter comme workaround:</p>
</blockquote>
<pre tabindex="0"><code>import nltk
import ssl

try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    ssl._create_default_https_context = _create_unverified_https_context

# nltk.download(&#39;stopwords&#39;)
nltk.download(&#39;...&#39;)
</code></pre>
            </div>
        </article>

        <hr />

        <div class="post-info">
                <p>
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="https://leandeep.com/tags/nlp">NLP</a></span><span class="tag"><a href="https://leandeep.com/tags/machine-learning">Machine Learning</a></span><span class="tag"><a href="https://leandeep.com/tags/ngrams">Ngrams</a></span>
                </p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>665 Mots</p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>03 sept.. 2019</p>
        </div>

        
            <div class="pagination">
                <div class="pagination__title">
                    <span class="pagination__title-h"></span>
                    <hr />
                </div>

                <div class="pagination__buttons">
                    
                        <span class="button previous">
                            <a href="https://leandeep.com/installer-en-30-secondes-un-bon-vieux-ftp-sur-ubuntu/">
                                <span class="button__icon">‚Üê</span>
                                <span class="button__text">Installer en 30 secondes un bon vieux FTP sur Ubuntu</span>
                            </a>
                        </span>
                    

                    
                        <span class="button next">
                            <a href="https://leandeep.com/installer-gitlab-ce-avec-un-runner-sur-centos-7-sur-aws/">
                                <span class="button__text">Installer Gitlab CE avec un runner sur Centos 7 sur AWS</span>
                                <span class="button__icon">‚Üí</span>
                            </a>
                        </span>
                    
                </div>
            </div>
        

        
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>Built by <a href="https://www.linkedin.com/in/oliviereeckhoutte/">Olivier Eeckhoutte</a>,
                Freelance @ LeanDeep <a href="https://leandeep.com/about/">(üçÉ company)</a></span>
            <span>Siret: 83825337500011</span>
            <span><a href="https://leandeep.com/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss">
                        <path d="M4 11a9 9 0 0 1 9 9"></path>
                        <path d="M4 4a16 16 0 0 1 16 16"></path>
                        <circle cx="5" cy="19" r="1"></circle>
                    </svg></a></span>
        </div>
    </div>
</footer>
            
        </div>

        




<script type="text/javascript" src="https://leandeep.com/bundle.min.c184f8481b5847ad1a7d8aa775944fa063f118cb4df68f4eaa3826a2a2e16b26a1ad798f5160210f265c6fbb9a5f19b953fed066ae1ed1092d1858bcff13ae92.js" integrity="sha512-wYT4SBtYR60afYqndZRPoGPxGMtN9o9OqjgmoqLhayahrXmPUWAhDyZcb7uaXxm5U/7QZq4e0QktGFi8/xOukg=="></script>







    </body>
</html>
